{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to remake the variation excel sheet so that it creates distinct lines for each type of mutation \n",
    "and the location that it affects. So if it is intergenic, I have a line for upstream and the downstream gene\n",
    "or if its in the promoter of one gene and in the coding sequence of another these get seperate lines.\n",
    "I will need to manually add some things like whether or not the gene is affected or not by the mutation type\n",
    "whether the innoculum has it \n",
    "\n",
    "then for every line I make a promoter region, an downstream region and coding region primers. \n",
    "\n",
    "if there is a similar mutation in the region, then the same effect will be predicted e.g. if there \n",
    "is an insertion of % 3 != 0 in the innoculum within the coding region, then the gene is off \n",
    "if there is not, then the gene is back ON (as in the reference)\n",
    "\n",
    "if there is a different mutation, i will need to interpret it on the fly i.e. and snv in the coding region will need to be compared by in-silico translation whereas an indel (%3!=0) can be assumed to cause a frameshift, where as ANYthing in a promoter can be interpreted as an \"unkown\" effect \n",
    "\n",
    "I then need to determine whether the homolog in the peruvian genome corresponds to a known gene \n",
    "I will need to add the predicted length of the protein and compare it to original \n",
    "\n",
    "then I need to go back and determine whether there reference version is functional or not \n",
    "\n",
    "\n",
    "BUG TO DO: in Crofts_Relapse_wo_Region for pseD is incorrect? \n",
    "\n",
    "FEATURE TO ADD: populations expected not to have the wild type \n",
    "\n",
    "Feature to add: see if existing clusters homologs in the abundances in which we detect them \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The purpose of this notebook is to analyze the variants reported in Crofts et al. It seeks to reproduce their results with their own data, as well as in recrudescent strains collected from community acquired infections.\n",
    "\n",
    "#### In this first block, we will simply import the packages we need to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will ultimately call SNPs using the library of a relapsed isolate mapped against the draft assembly of a paired primary infection. This differs from the SNP calling pipeline used in the Crofts et al paper becuase they call SNPs against a complete reference genome. To gauge the posibility for false positives and negatives, we simulated a draft assembly of their complete genome using `wgsim`. In this step, we will examine its quality using BLASTn.\n",
    "\n",
    "#### From this we can see that the reference genome fully covered identically by the assembly. We can also see that there are a number of smaller contigs that are repeatedly aligned with the reference at different points. Some of these repeats stich over portion of the assembly that would be otherwised uncovered (eg. contigs 10 and 12) but others map to regions that are already covered (eg. contig 18). The latter implies that there may be a gene that is repeated, but which has diverged internally, which can aberrations in coverage in the region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_assem = \"/Volumes/KeithSSD/variant-calling/simulated_reference/CG8421_sim_contigs_fixed.fa\"\n",
    "sim_check = \"/Volumes/KeithSSD/variant-calling/simulated_reference/simulation_matches.txt\"\n",
    "complete_ref = \"/Volumes/KeithSSD/variant-calling/ref_fastas/CG8421_bb.fa\"\n",
    "complete_db = \"/Volumes/KeithSSD/variant-calling/simulated_reference/complete_ref_db\"\n",
    "\n",
    "print('\\nOUTFMT=\"6 qseqid sseqid pident length qcovs mismatch gapopen qstart qend qlen sstart send slen evalue bitscore sstrand\"\\n')\n",
    "print(\"makeblastdb -in {} -dbtype nucl -out {}\\n\".format(complete_ref, complete_db))\n",
    "bp1 = 'blastn -evalue 1e-3 -db {} -query {} -outfmt '.format(complete_db, sim_assem)\n",
    "bp2 = '\"$OUTFMT\" -qcov_hsp_perc 90 -out {} -num_threads 3\\n\\n'.format(sim_check)\n",
    "print(bp1+bp2)\n",
    "\n",
    "blast_cols1 = ['primer', 'sseqid', 'pident', 'length', 'qcov', 'mismatch', 'gapopen', 'qstart', 'qend',\n",
    "              'qlen', 'sstart', 'send', 'slen', 'evalue', 'bitscore', 'sstrand']\n",
    "sim_check_df = pd.read_csv(sim_check, sep=\"\\t\", header=None)\n",
    "sim_check_df.columns = blast_cols1\n",
    "#sim_check_df['reversed'] = sim_check_df[['sstart', 'send']].apply(lambda x: min(x), axis=1)\n",
    "sim_check_df['sstart2'] = sim_check_df[['sstart', 'send']].apply(lambda x: min(x), axis=1)\n",
    "sim_check_df['send2'] = sim_check_df[['sstart', 'send']].apply(lambda x: max(x), axis=1)\n",
    "sim_check_df.sort_values(by='sstart2', inplace=True)\n",
    "\n",
    "#sim_check_df.loc[[7, 11, 4, 13, 6, 14, 10, 21, 20, 23, 12], ['primer', 'sseqid', 'pident', 'length', 'qcov', 'mismatch', 'gapopen', 'qstart', 'qend',\n",
    "#              'qlen', 'sstart2', 'send2', 'sstrand']]\n",
    "\n",
    "sim_check_df.loc[:, ['primer', 'sseqid', 'pident', 'length', 'qcov', 'mismatch', 'gapopen', 'qstart', 'qend',\n",
    "              'qlen', 'sstart2', 'send2', 'sstrand']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we read in a Genbank file for the reference strain (CG8421) from Crofts et al. We want a dataframe containing gene feature positions, annotations, protein and DNA sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_file = \"/Volumes/KeithSSD/variant-calling/refs/GCF_000171795.2_ASM17179v2_genomic.gbff\"\n",
    "\n",
    "gb_records = {i.name:i for i in list(SeqIO.parse(open(gb_file,\"r\"), \"genbank\"))}\n",
    "\n",
    "gb_subdfs = []\n",
    "for chrom, gb_record in gb_records.items():\n",
    "    feat_idx = list(range(len(gb_record.features)))\n",
    "    exp_cols = set(['molecule', 'type', 'strand', 'start', 'end', 'sequence'])\n",
    "    exp_cols.update([k for i in feat_idx for k in gb_record.features[i].qualifiers.keys()])\n",
    "    gb_subdf = pd.DataFrame(index=feat_idx, columns=exp_cols)\n",
    "    for i in gb_subdf.index:\n",
    "        feature_i = gb_record.features[i]\n",
    "        gb_subdf.at[i, 'molecule'] = chrom\n",
    "        gb_subdf.at[i, 'type'] = feature_i.type\n",
    "        gb_subdf.at[i, 'strand'] = feature_i.location.strand\n",
    "        gb_subdf.at[i, 'start'] = int(feature_i.location.start)\n",
    "        gb_subdf.at[i, 'end'] = int(feature_i.location.end)\n",
    "        # This already does reverse complementing of crick strand features\n",
    "        gb_subdf.at[i, 'sequence'] = str(feature_i.extract(gb_record.seq))\n",
    "        for qk, qv in feature_i.qualifiers.items():\n",
    "            if len(qv) != 1:\n",
    "                qv = \", \".join(qv)\n",
    "            gb_subdf.at[i, qk] = qv[0]\n",
    "    gb_subdfs.append(gb_subdf.copy())\n",
    "    print(\"Parsed {} features from {} to a dataframe size {}\".format(len(feat_idx), chrom, gb_subdf.shape))\n",
    "    \n",
    "gb_df = pd.concat(gb_subdfs, axis=0, ignore_index=True)\n",
    "print(\"Full genbank dataframe sized {}\".format(gb_df.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we will read in some information about the mutations detected in the Crofts study taken from their supplement. This block contains a sanity test whereby we match the reference allele nucleotides specified at locii where deletions and SNVs were discovered can be located in the genome sequence we extracted from our Genbank file. \n",
    "\n",
    "#### Positional information about insertions and multinucleotide deletions is given as a string with two coordinates seperated by a carot or two periods. We will reformat this information into a python tuple containing integers so that it can be used to index the string object containing the full genome sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_file = '/Volumes/KeithSSD/variant-calling/misc_data/Crofts_Gene_Variants.xlsx'\n",
    "var_df = pd.read_excel(var_file, sheet_name='Spec_Variants', index_col=None)\n",
    "var_df['Variant Position'] = var_df['Variant Position'].astype(str).str.replace(\"\\.\\.\", ',')\n",
    "var_df['Variant Position'] = var_df['Variant Position'].str.replace(\"^\", ',')\n",
    "\n",
    "full_genome = gb_df.loc[0, 'sequence']\n",
    "\n",
    "lociii = []\n",
    "for var_i in var_df.index:\n",
    "    locii = str(var_df.loc[var_i, 'Variant Position']).replace(\"*\", \"\").split(\",\")\n",
    "    locii = tuple([int(i) for i in locii])\n",
    "    lociii.append(locii)\n",
    "    detectable_muts = var_df.loc[var_i, 'Variant Type'] in ['Deletion', 'SNV']\n",
    "    if len(locii) == 1 and detectable_muts:\n",
    "        assert full_genome[locii[0]-1] == var_df.at[var_i, 'Reference Sequence']\n",
    "    elif len(locii) == 2 and detectable_muts:\n",
    "        assert full_genome[locii[0]-1:locii[1]] == var_df.at[var_i, 'Reference Sequence']\n",
    "\n",
    "var_df['var_locii'] = pd.Series(index=var_df.index, data=lociii)\n",
    "\n",
    "var_df['Variant Location'] = var_df['Variant Location'].apply(lambda x: x.strip().upper())\n",
    "\n",
    "var_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code block performs the Fisher Exact test and bonferroni correction on the variations observed. Individually, very few rise to the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "var_file = '/Volumes/KeithSSD/variant-calling/misc_data/Crofts_Variants_Pooled.xlsx'\n",
    "pooled_var_df = pd.read_excel(var_file, sheet_name='Sheet1', index_col=None)\n",
    "\n",
    "col1 = 'Relapse Sample Count (out of 6)'\n",
    "col2 = 'Relapse w/o Variant'\n",
    "col3 = 'Primary Infection Samples with Variant(s) (out of 43)'\n",
    "col4 = 'Primary Infections wo Variant'\n",
    "\n",
    "pooled_var_df[col2] = 6 - pooled_var_df[col1]\n",
    "pooled_var_df[col4] = 43 - pooled_var_df[col3]\n",
    "\n",
    "oddsratio_fxn = lambda x: fisher_exact([[x[0], x[2]], [x[1], x[3]]])[0]\n",
    "pvalue_fxn = lambda x: fisher_exact([[x[0], x[2]], [x[1], x[3]]])[1]\n",
    "\n",
    "pooled_var_df['oddsratio'] = pooled_var_df[[col1, col2, col3, col4]].apply(oddsratio_fxn,  axis=1)\n",
    "pooled_var_df['p_val'] = pooled_var_df[[col1, col2, col3, col4]].apply(pvalue_fxn,  axis=1)\n",
    "\n",
    "corr_p_vals = multipletests(pooled_var_df['p_val'].values, method='bonferroni', alpha=0.05)\n",
    "pooled_var_df['correct_p_val'] = pd.Series(corr_p_vals[1], index=pooled_var_df.index)\n",
    "pooled_var_df[\"Relapse w/Variant\"] = pooled_var_df[col1]\n",
    "pooled_var_df[\"Primary Infection w/Variant\"] = pooled_var_df[col3]\n",
    "\n",
    "pooled_var_df['Reported Significant Associations'] = pooled_var_df['Variant Association with Human Disease State as Determined by Fisher Exact Test (FDR p-value <0.05)']\n",
    "\n",
    "pooled_var_df[['Annotation', 'Reported Significant Associations', \"Relapse w/Variant\", col2, \n",
    "               \"Primary Infection w/Variant\", col4, 'p_val', 'oddsratio', 'correct_p_val']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I noticed that some of the locus tags listed in their Supplemental Data file have been replaced with new ones in the latest revision of the CG8421 genome, so this code goes through the variation dataframe and makes those same updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix formatting\n",
    "var_df['affected'] = var_df[\"Affected CG8421 Gene(s)\"].str.replace(\", Promoter of \", \" & \")\n",
    "var_df['affected'] = var_df['affected'].apply(lambda x: x.split(\" & \"))\n",
    "\n",
    "# manually verified = [12, 13, 14, 25, 26, 29, 30, 41, 42]\n",
    "updated_tags = {\"CJ8421_RS03020\": \"CJ8421_RS08905\", \n",
    "                \"CJ8421_RS03305\": \"CJ8421_RS08915\", \n",
    "                \"CJ8421_RS06480\": \"CJ8421_RS08860\",\n",
    "                'CJ8421_RS06580': 'CJ8421_RS08870',\n",
    "                'CJ8421_RS06585': 'CJ8421_RS08870',\n",
    "                \"CJ8421_RS07090\": \"CJ8421_RS07085\"}\n",
    "def fix_tags(x):\n",
    "    y = []\n",
    "    for ip in x:\n",
    "        i = ip.strip()\n",
    "        if not i in updated_tags.keys():\n",
    "            y.append(i)\n",
    "        else:\n",
    "            y.append(updated_tags[i])\n",
    "    return list(set(y))\n",
    "\n",
    "var_df['affected_2'] = var_df['affected'].apply(fix_tags)\n",
    "changes_made = var_df[['affected', 'affected_2']].apply(lambda x: len(set(x[0]) - set(x[1])), axis=1).sum()\n",
    "print(\"{} locus tags were either changed or removed\".format(changes_made))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will confirm the adjacency of each mutation and its affected gene. In the same script, we can add a column to indicate whether the mutation was detected within or between genes and another to collect the identifiers for additional nearby features that may add support for whether a paralog to the target region in the CG8421 genome is detected in our Peruvian isolates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all gene & cds are 1:1 in the records & location searches will be hampered by the source rec\n",
    "ngb_df = gb_df.copy()[~gb_df['type'].isin(['gene', 'source'])]\n",
    "\n",
    "var_cols = ['Variant Position', \"Variant Type\", \"Reference Sequence\", \"Variant Allele\"]\n",
    "\n",
    "anchor_dicts, anch_counter = {}, 0 \n",
    "for var_i in var_df.index:\n",
    "    mut_pos = var_df.loc[var_i, 'var_locii'][0]\n",
    "    mut_us, mut_ds = mut_pos + 150, mut_pos - 150\n",
    "    coding_bool = ngb_df['type'] == 'CDS'\n",
    "    within_bool = (ngb_df['start'] <= mut_pos) & (ngb_df['end'] >= mut_pos) & (coding_bool)\n",
    "    downstream = (ngb_df['start'] > mut_pos) & (ngb_df['start'] <= mut_us) & (coding_bool)\n",
    "    upstream = (ngb_df['end'] < mut_pos) & (ngb_df['end'] >= mut_ds) & (coding_bool)\n",
    "    sensor_bools = [(downstream) & (ngb_df['strand'] == 1),\n",
    "                    (upstream) & (ngb_df['strand'] == 1), \n",
    "                    (downstream) & (ngb_df['strand'] == -1),\n",
    "                    (upstream) & (ngb_df['strand'] == -1), \n",
    "                    within_bool]\n",
    "     \n",
    "    sensor_types = ['upstream', 'downstream', 'upstream', 'downstream', 'in_CDS']\n",
    "    keys_needed = ['locus_tag', 'start', 'end', 'strand', 'product', 'sequence', 'pseudo']\n",
    "    all_pinged = within_bool.sum()+downstream.sum()+upstream.sum()\n",
    "     \n",
    "    print(\"{}. {} possible genes affected by mutation at {}\".format(var_i, all_pinged, mut_pos))\n",
    "    #print(\"\\t= {}\".format(var_df.loc[var_i, 'affected_2']))\n",
    "    #print(\"{}, {}\".format(mut_pos, ngb_df.loc[within_bool | upstream | downstream, keys_needed].T))\n",
    "    #input()\n",
    "     \n",
    "    for sensor, senseType in zip(sensor_bools, sensor_types):\n",
    "        if sensor.sum() > 0:\n",
    "            sensee = ngb_df.loc[sensor, keys_needed]\n",
    "            for hit_i in sensee.index:\n",
    "                pos_dict = {'variation_index':var_i, 'mut_start':mut_pos, 'var_location': senseType}\n",
    "                for vc in var_cols:\n",
    "                    pos_dict[vc] = var_df.loc[var_i, vc]\n",
    "                for kn in keys_needed:\n",
    "                    pos_dict[kn] = sensee.loc[hit_i, kn]\n",
    "                if pos_dict['strand'] == 1:\n",
    "                    pos_dict['upstream_seq'] = full_genome[pos_dict['start']-20-1:pos_dict['start']+180]\n",
    "                    pos_dict['downstream_seq'] = full_genome[pos_dict['end']-20-1:pos_dict['end']+180]\n",
    "                    pos_dict['upstream_region'] = (pos_dict['start']-20, pos_dict['start']+180)\n",
    "                    pos_dict['downstream_region'] = (pos_dict['end']-20, pos_dict['end']+180)\n",
    "                else:\n",
    "                    pos_dict['upstream_seq'] = full_genome[pos_dict['end']-20-1:pos_dict['end']+180]\n",
    "                    pos_dict['downstream_seq'] = full_genome[pos_dict['start']-20-1:pos_dict['start']+180]\n",
    "                    pos_dict['upstream_region'] = (pos_dict['end']-20, pos_dict['end']+180)\n",
    "                    pos_dict['downstream_region'] = (pos_dict['start']-20, pos_dict['start']+180)\n",
    "                anchor_dicts[anch_counter] = pos_dict\n",
    "                anch_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are going to add some genes we are fans of and some random genes to search for just to get a sense of the range of matches observed in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To determine if our mutation is conserved we will look for the interrupted gene, as well as some nearby genes to determine how conserved the region is. To do this, we will extract the following anchor sequences per feature:\n",
    "#### -  75 nucleotides on either side of mutation \n",
    "#### -  Gene sequences of 2 closest genes on either side and target gene\n",
    "#### - Protein sequences for all gene anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conserved_ = ngb_df['locus_tag'].isin(['CJ8421_RS08380', 'CJ8421_RS00020', 'CJ8421_RS00430',\n",
    "                                       'CJ8421_RS00435', 'CJ8421_RS02005', 'CJ8421_RS05885', \n",
    "                                       'CJ8421_RS06285', 'CJ8421_RS07345', 'CJ8421_RS07350', \n",
    "                                       'CJ8421_RS07355'])\n",
    "\n",
    "random_recs = ngb_df[coding_bool].sample(n=10, random_state=1)\n",
    "random_recs = random_recs.append(ngb_df[conserved_])\n",
    "\n",
    "print(\"Adding {} genes to search for, currently there are: {}\".format(len(random_recs), \n",
    "                                                                      len(anchor_dicts)))\n",
    "for hit_i in random_recs.index:\n",
    "    pos_dict = {}\n",
    "    for vc in var_cols+['variation_index', 'mut_start', 'var_location']:\n",
    "        pos_dict[vc] = np.nan\n",
    "    for kn in keys_needed:\n",
    "        pos_dict[kn] = ngb_df.loc[hit_i, kn]\n",
    "    if pos_dict['strand'] == 1:\n",
    "        pos_dict['upstream_seq'] = full_genome[pos_dict['start']-20-1:pos_dict['start']+180]\n",
    "        pos_dict['downstream_seq'] = full_genome[pos_dict['end']-20-1:pos_dict['end']+180]\n",
    "        pos_dict['upstream_region'] = (pos_dict['start']-20, pos_dict['start']+180)\n",
    "        pos_dict['downstream_region'] = (pos_dict['end']-20, pos_dict['end']+180)\n",
    "    else:\n",
    "        pos_dict['upstream_seq'] = full_genome[pos_dict['end']-20-1:pos_dict['end']+180]\n",
    "        pos_dict['downstream_seq'] = full_genome[pos_dict['start']-20-1:pos_dict['start']+180]\n",
    "        pos_dict['upstream_region'] = (pos_dict['end']-20, pos_dict['end']+180)\n",
    "        pos_dict['downstream_region'] = (pos_dict['start']-20, pos_dict['start']+180)\n",
    "    anchor_dicts[anch_counter] = pos_dict\n",
    "    anch_counter += 1\n",
    "\n",
    "anchor_df = pd.DataFrame(anchor_dicts).T\n",
    "print(\"Total of {} genes to discover i.e. {} primers\".format(len(anchor_df), len(anchor_df)*3))\n",
    "anchor_df = anchor_df.drop_duplicates()\n",
    "print(\"Total of {} genes to discover i.e. {} primers\".format(len(anchor_df), len(anchor_df)*3))\n",
    "anchor_df.loc[[19, 20], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we create a new folder for all data written to disk.\n",
    "#### Then we write out both anchor files into files called reference_anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_df['before_header'] = anchor_df['locus_tag'].apply(lambda x: \"{}-before\".format(x))\n",
    "anchor_df['after_header'] = anchor_df['locus_tag'].apply(lambda x: \"{}-after\".format(x))\n",
    "anchor_df['gene_header'] = anchor_df['locus_tag'].apply(lambda x: \"{}-gene\".format(x))\n",
    "\n",
    "def table_to_fasta(h_col, s_col, df, outname):\n",
    "    output = list(df[[h_col, s_col]].apply(lambda x: \">\"+str(x[0])+\"\\n\"+x[1], axis=1))\n",
    "    with open(outname, 'w') as fh:\n",
    "        fh.write(\"\\n\".join(output)+\"\\n\")\n",
    "    print(\"\\n\", outname, \"written\")\n",
    "    return\n",
    "\n",
    "data_dir = \"/Volumes/KeithSSD/variant-calling/variant_homologs\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "gene_file = os.path.join(data_dir, 'gene_primers.fa')\n",
    "\n",
    "col_pairs = [('downstream_seq', 'before_header'), \n",
    "             ('upstream_seq', 'after_header'), \n",
    "             ('sequence', 'gene_header')]\n",
    "\n",
    "outdf = pd.DataFrame(columns=['headers', 'nucleotides'], \n",
    "                     data=[list(x) for s, h in col_pairs for x in anchor_df[[h, s]].values]).drop_duplicates()\n",
    "\n",
    "table_to_fasta('headers', 'nucleotides', outdf, gene_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will read in a text file containing information on the primary infection and relapse isolate pairs identified in both our and the Crofts et al. studies.  We use this information, and two columns from the Crofts supplement to define a new column, each row of which contains a list of the isolates expected to have a mutation in a given location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = \"/Volumes/KeithSSD/CA_V3/data/10_functional_enrichment_final/enrichment_categories.txt\"\n",
    "metadata_df = pd.read_csv(metadata_file, sep=\"\\t\", index_col=0)\n",
    "relapse_pair_isos = list(metadata_df[metadata_df['primary_v_relapse'].notnull()].index)\n",
    "print(\"{} relapse pair strains identified, including:\\n\\t{}\\n\".format(len(relapse_pair_isos), relapse_pair_isos))\n",
    "all_crofts_ = [i for i in metadata_df.index if 'crofts' in i]\n",
    "relapse_pair_isos = list(set(relapse_pair_isos + all_crofts_))\n",
    "print(\"At {} after adding the rest of Crofts isolates.\".format(len(relapse_pair_isos)))\n",
    "\n",
    "def fix_pop_col(x):\n",
    "    f1 = lambda x: x.replace(\"Relapse 8.\", \"crofts_8R\")\n",
    "    f2 = lambda x: x.replace(\"Relapse 1\", \"crofts_1R\")\n",
    "    f3 = lambda x: x.replace(\"Relapse 9\", \"crofts_31R\")\n",
    "    f4 = lambda x: x.replace(\"Relapse 4\", \"crofts_9R\")\n",
    "    f5 = lambda x: x.replace(\"Relapse 31\", \"crofts_4R\")\n",
    "    f6 = lambda x: \"crofts_\" + x if 'D' in x else x\n",
    "    f7 = lambda x: f6(f1(f2(f3(f4(f5(x.strip()))))))\n",
    "    return list(map(f7, x.split(\",\")))\n",
    "\n",
    "cn1 = 'Isolate Population Samples That Differed From Inoculum'\n",
    "cn2 = 'Variant Called in Inoculum or Isolate Population?'\n",
    "\n",
    "var_df['Isos_unlike_Innoc'] = var_df[cn1].apply(fix_pop_col)\n",
    "f8 = lambda x: set(['crofts_innoc']) if x[0] == 'Inoculum' else set(x[1]).intersection(set(relapse_pair_isos))\n",
    "f9 = lambda x: set(relapse_pair_isos).intersection(x[1]) if x[0] == 'Inoculum' else set()\n",
    "var_df['Samps_w_Mutation'] = var_df[[cn2, 'Isos_unlike_Innoc']].apply(f8, axis=1)\n",
    "var_df['Samps_off_Target'] = var_df[[cn2, 'Isos_unlike_Innoc']].apply(f9, axis=1)\n",
    "\n",
    "for add_cols_ in ['Samps_w_Mutation', 'Samps_off_Target']:\n",
    "    anchor_df[add_cols_] = anchor_df['variation_index'].map(var_df[add_cols_].to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now import full genome sequences and predicted protein sequences. The protein tables were exported from an'vio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_dir = \"/Volumes/KeithSSD/CA_V3/data/03_CNTG_DBs_final\"\n",
    "prot_paths = [os.path.join(prot_dir, i+\"_Proteins.txt\") for i in relapse_pair_isos]\n",
    "assert sum([os.path.exists(i) for i in prot_paths]) == len(prot_paths)\n",
    "pos_paths = [os.path.join(prot_dir, i+\"_GenePositions.txt\") for i in relapse_pair_isos]\n",
    "assert sum([os.path.exists(i) for i in pos_paths]) == len(pos_paths)\n",
    "contigs_dir = \"/Volumes/KeithSSD/CA_V3/data/02_ASSEMBLY_final\"\n",
    "assem_paths = [os.path.join(contigs_dir, i+\"_contigs_fixed.fa\") for i in relapse_pair_isos]\n",
    "assert sum([os.path.exists(i) for i in assem_paths])\n",
    "\n",
    "relapse_pair_isos.append('CG8421_sim')\n",
    "pos_paths.append('/Volumes/KeithSSD/variant-calling/simulated_reference/CG8421_GenePositions.txt')\n",
    "prot_paths.append('/Volumes/KeithSSD/variant-calling/simulated_reference/CG8421_Proteins.txt')\n",
    "assem_paths.append('/Volumes/KeithSSD/variant-calling/simulated_reference/CG8421_sim_contigs_fixed.fa')\n",
    "\n",
    "prot_dict = {}\n",
    "print(\"Reading protein and gene position files\")\n",
    "for samp, prot_f, pos_f in zip(relapse_pair_isos, prot_paths, pos_paths):\n",
    "    pos_df = pd.read_csv(pos_f, sep=\"\\t\", index_col=0)\n",
    "    prot_df = pd.read_csv(prot_f, sep=\"\\t\", index_col=0)\n",
    "    protpos_df = prot_df.join(pos_df, on=None, how='outer').reset_index()\n",
    "    assert len(pos_df) == len(prot_df) == len(protpos_df)\n",
    "    protpos_df['genome_name'] = pd.Series(index=protpos_df.index, \n",
    "                                       data=[samp]*len(protpos_df))\n",
    "    prot_dict[samp] = protpos_df.copy()\n",
    "\n",
    "def read_contig_file(contig_path, samp_name):\n",
    "    header_lines, contig_sequences = [], []\n",
    "    with open(contig_path, 'r') as fh:\n",
    "        for line in fh:\n",
    "            if line.startswith(\">\"):\n",
    "                if len(header_lines) > 0:\n",
    "                    contig_sequences.append(temp_sequence)\n",
    "                header_lines.append(line[1:].strip())\n",
    "                temp_sequence = \"\"\n",
    "            else:\n",
    "                temp_sequence += line.strip()\n",
    "    \n",
    "    contig_sequences.append(temp_sequence)\n",
    "    if len(header_lines) != len(contig_sequences):\n",
    "        print(len(header_lines), len(contig_sequences))\n",
    "    assert len(header_lines) == len(contig_sequences)\n",
    "    \n",
    "    genome_size = len(\"\".join(contig_sequences))\n",
    "    new_headers = [samp_name + \"-\" + j + \"-\" + 'x' for i, j in enumerate(header_lines)]\n",
    "    contig_df = pd.DataFrame(data=[header_lines, new_headers, contig_sequences], index=[\"old_ids\", 'new_ids', 'sequence']).T\n",
    "    return contig_df\n",
    "\n",
    "print(\"Reading assemblies\")\n",
    "contigs = {i:read_contig_file(j, i) for i, j in zip(relapse_pair_isos, assem_paths)}\n",
    "\n",
    "all_prot_df = pd.concat(prot_dict.values(), ignore_index=True, axis=0)\n",
    "hdr_fxn = lambda x: \"-\".join([str(i) for i in x])\n",
    "all_prot_df['new_ids'] = all_prot_df[['genome_name', 'contig', 'gene_callers_id']].apply(hdr_fxn, axis=1)\n",
    "print(\"Total # proteins read {}\".format(len(all_prot_df)))\n",
    "\n",
    "all_cntg_df = pd.concat(contigs.values(), ignore_index=True, axis=0)\n",
    "print(\"Total # contigs read {}\".format(len(all_cntg_df)))\n",
    "\n",
    "print(all_cntg_df.tail())\n",
    "print(all_prot_df.tail())\n",
    "\n",
    "gene_cluster_file = '../../CA_V3/data/07_PAN_GENOME_final/Trial_2/gene_clusters.txt'\n",
    "cluster_df = pd.read_csv(gene_cluster_file, sep=\"\\t\")\n",
    "\n",
    "cluster_df['mapper'] = cluster_df[['gene_caller_id', 'genome_name']].apply(tuple, axis=1)\n",
    "clust_map = {k:v for k, v in list(cluster_df[['mapper', 'gene_cluster_id' ]].values)}\n",
    "\n",
    "all_prot_df['mapper'] = cluster_df[['gene_caller_id', 'genome_name']].apply(tuple, axis=1)\n",
    "all_prot_df['gene_cluster_id'] = all_prot_df['mapper'].map(clust_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will add the reference genome to these dataframes as a positive control and then we will print the correctly formatted commands to blast in the Terminal. We want to enforce 45% query coverage for both exact and full gene sequences. For the latter, it is because no matter where a frameshift occurs in a protein, at least 50% of one of the frames will still match the original. For the former, it is because the mutation is expected in the middle of the sequence and if the alignment decides to break, instead of pushing through the mismatch, a little under 50% of it should still be matched. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index = list(gb_df[gb_df['type'] == 'source'].index)\n",
    "assert len(src_index) == 1\n",
    "header_pfx = \"-\".join(list(gb_df.loc[src_index[0], ['strain', 'molecule']]) + ['x'])\n",
    "ref_row = pd.DataFrame({all_cntg_df.shape[0]:{'old_ids':\"\", 'new_ids':header_pfx, 'sequence': full_genome}}).T\n",
    "all_cntg_df3 = all_cntg_df.append(ref_row, sort=True)\n",
    "all_cntg_df2 = all_cntg_df3[['new_ids', 'sequence']].dropna()\n",
    "print(\"Contigs number upped from {} to {}\".format(len(all_cntg_df), len(all_cntg_df2)))\n",
    "\n",
    "nt_ref_file = os.path.join(data_dir, 'contig_pile.fa')\n",
    "ntdb = nt_ref_file.replace(\".fa\", \"_db\")\n",
    "gene_out = os.path.join(data_dir, 'gene_hits.txt')\n",
    "table_to_fasta('new_ids', 'sequence', all_cntg_df2, nt_ref_file)\n",
    "\n",
    "print('\\nOUTFMT=\"6 qseqid sseqid pident length qcovs mismatch gapopen qstart qend qlen sstart send slen evalue bitscore sstrand\"\\n')\n",
    "print(\"makeblastdb -in {} -dbtype nucl -out {}\\n\".format(nt_ref_file, ntdb))\n",
    "print('tblastx -query_gencode 11 -db_gencode 11 -evalue 1e-3 -db {} -query {} -outfmt \"$OUTFMT\" -qcov_hsp_perc 45 -out {} -num_threads 3 \\n'.format(ntdb, gene_file, gene_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After running BLAST, we read back in the results. BLAST can be tricky because if you allow for very lenient alignment parameters, the program will not find optimal alignments, however if you err on the side of conservatism, you risk false negatives. With that in mind, we made sure to use BLAST command parameters and post-processing filtering that allowed the program to get a minimum of 1 perfect match. We then set an identity limit by cutting and pasting the primers into the BLAST web search tool and performing tblasx against the RefSeq Campylobacter coli genome. The hits and misses were obvious and so we set an addtional, such that all substandard alignments were removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def best_pairs(df):\n",
    "    primers_ = df['primer'].unique()\n",
    "    genomes_ = df['genome_name'].unique()\n",
    "    subdfs = [df[(df['primer'] == p) & (df['genome_name'] == g)] for p, g in product(primers_, genomes_)]\n",
    "    subdfs2 = [df[df.evalue == df.evalue.min()] for df in subdfs]\n",
    "    bestidxs = [i for df in subdfs2 for i in df[df.pident_adj == df.pident_adj.max()].index]\n",
    "    df2 = df.loc[bestidxs, :].copy()\n",
    "    df3 = df2[df2.pident > 80.0 ]\n",
    "    check_ = [df3[(df3['primer'] == p) & (df3['genome_name'] == 'CG8421')].shape[0] > 0 for p in primers_]\n",
    "    assert set(check_) == set([True])\n",
    "    return df3.copy()\n",
    "\n",
    "blast_cols = ['primer', 'sseqid', 'pident', 'length', 'qcov', 'mismatch', 'gapopen', 'qstart', 'qend',\n",
    "              'qlen', 'sstart', 'send', 'slen', 'evalue', 'bitscore', 'sstrand']\n",
    "new_cols = ['genome_name', 'contig', 'gene_callers_id', 'pident_adj', 'evalue']\n",
    "extra_cols = set(blast_cols[5:]) - set(new_cols)\n",
    "new_order = blast_cols[:5] + new_cols + list(extra_cols) + ['present', 'sstart2', 'send2']\n",
    "\n",
    "hit_df = pd.read_csv(gene_out, sep=\"\\t\", header=None)\n",
    "hit_df.columns = blast_cols\n",
    "hit_df['genome_name'] = hit_df['sseqid'].apply(lambda x: x.split(\"-\")[0])\n",
    "hit_df['contig'] = hit_df['sseqid'].apply(lambda x: x.split(\"-\")[1])\n",
    "hit_df['gene_callers_id'] = hit_df['sseqid'].apply(lambda x: x.split(\"-\")[2])\n",
    "hit_df['qcov'] = abs(hit_df['qend'] - hit_df['qstart']) / hit_df['qlen']\n",
    "hit_df['sstart2'] = hit_df[['sstart', 'send']].apply(lambda x: min(x), axis=1)\n",
    "hit_df['send2'] = hit_df[['sstart', 'send']].apply(lambda x: max(x), axis=1)\n",
    "hit_df['pident_adj'] = hit_df['pident']*hit_df['qcov']\n",
    "hit_df['present'] = pd.Series(index=hit_df.index, data=[1]*len(hit_df))\n",
    "print(\"Raw BLAST hits total {} w/ {} columns\".format(len(hit_df), hit_df.shape[1]))\n",
    "assert set(new_order) == set(hit_df.columns)\n",
    "blast_df = best_pairs(hit_df[new_order])\n",
    "blast_df['locus_tag'] = blast_df['primer'].apply(lambda x: x.split(\"-\")[0])\n",
    "blast_df['primer_position'] = blast_df['primer'].apply(lambda x: x.split(\"-\")[1])\n",
    "print(\"\\tAfter keeping only best pairs, we are left with {}\".format(blast_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we compile both types of BLAST searches and pivot them into a presence/absence table with primers in the columns and genomes in the rows. We will also make a similarly shaped table with all the location information for each hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crofts_genomes = list(filter(lambda x: x.startswith(\"crofts\"), blast_df['genome_name'].unique()))\n",
    "my_genomes = [i for i in set(blast_df['genome_name']) if not i.startswith(\"CG8421\") and not i in crofts_genomes]\n",
    "\n",
    "def fix_aggs(x):\n",
    "    x.index.name = None\n",
    "    x.columns = x.columns.droplevel()\n",
    "    x.columns.name = None\n",
    "    return x\n",
    "\n",
    "# Create a location to record the # of valid homologs per primer\n",
    "pre_homolog = blast_df[['genome_name', 'primer', 'present']].groupby(['genome_name', 'primer'])\n",
    "homolog_df = fix_aggs(pre_homolog.agg(np.sum).unstack(level=-1))\n",
    "\n",
    "# Create permanent + temporary locations to record the corresponding contig and location values \n",
    "loc_cols = ['sseqid', 'sstart2', 'send2', 'qstart', 'qend', 'pident']\n",
    "blast_df['loc_info'] = blast_df[loc_cols].apply(tuple, axis=1)\n",
    "pre_loc = blast_df[['genome_name', 'primer', 'loc_info']].groupby(['genome_name', 'primer'])\n",
    "location_df = fix_aggs(pre_loc.agg(list).unstack(level=-1))\n",
    "\n",
    "## Create a location for e-values and set permissive defaults \n",
    "pre_eval = blast_df[['genome_name', 'primer', 'evalue']].groupby(['genome_name', 'primer'])\n",
    "evalue_df = fix_aggs(pre_eval.agg(list).unstack(level=-1))\n",
    "\n",
    "searched_for = homolog_df.shape[0]*homolog_df.shape[1]\n",
    "detected_n = searched_for - homolog_df.isnull().sum().sum()\n",
    "print(\"Total possible homologs = {}, Total valid ones detected = {}\".format(searched_for, detected_n))\n",
    "\n",
    "coldOrder = ['CG8421', 'CG8421_sim'] + crofts_genomes + my_genomes\n",
    "assert set(coldOrder) - set(evalue_df.index) == set()\n",
    "\n",
    "homolog_group_order = []\n",
    "for aff_gene in anchor_df['locus_tag'].unique():\n",
    "    homolog_group_order += [aff_gene+'-before', aff_gene+'-gene', aff_gene+'-after']\n",
    "\n",
    "assert set(homolog_group_order) == set(homolog_df.columns)\n",
    "homolog_df.loc[coldOrder, homolog_group_order].to_csv(\"/Volumes/KeithSSD/variant-calling/misc_data/homologs.txt\", sep=\"\\t\")\n",
    "evalue_df.loc[coldOrder, homolog_group_order].to_csv(\"/Volumes/KeithSSD/variant-calling/misc_data/evalues.txt\", sep=\"\\t\")\n",
    "location_df.loc[coldOrder, homolog_group_order].to_csv(\"/Volumes/KeithSSD/variant-calling/misc_data/locations.txt\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we know where the hits are, we need to integrate them with the variation dataframe file. We go through our BLAST hits by variant region and check that either the full gene or the exact region in which a variation was detected was on the same contig as at least one of the genes affected by that variation. Sometimes the variation is within the gene, and so this is a redundant check, but many times they are not. We also go through and check to see if BLAST was able to locate an exact copy of the wild-type allele (found in the reference genome) in the isolate assembly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wt(gnsr, ra):\n",
    "    cntg_df, gnc = all_cntg_df3.copy(), list(gnsr)\n",
    "    if type(ra) != float:\n",
    "        cntg_match = cntg_df['new_ids'] == gnc[0]\n",
    "        if cntg_match.sum() == 1:\n",
    "            cntg_seq = cntg_df.loc[cntg_match, 'sequence'].values[0]\n",
    "            for sub_seq in ra:\n",
    "                if cntg_seq.find(sub_seq, gnc[1]-20, gnc[2]+20) != -1:\n",
    "                    return True\n",
    "            return False\n",
    "        else:\n",
    "            raise ValueError(\"what contig from where: {}\".format(gnc))\n",
    "    else:\n",
    "        return ra\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna\n",
    "\n",
    "def seq_array(ra):\n",
    "    ra2 = Seq(ra, generic_dna)\n",
    "    ra_ls = [ra]\n",
    "    ra_ls.append(str(ra2.reverse_complement()))\n",
    "    ra_ls.append(str(ra2.complement()))\n",
    "    ra_ls.append(str(ra2.reverse_complement().complement()))\n",
    "    return ra_ls\n",
    "\n",
    "## Repeat the iteration from before \n",
    "data_prod_1 = []\n",
    "\n",
    "pop_cols = ['has_wildtype', 'has_homolog']\n",
    "for pop_colx in pop_cols:\n",
    "    anchor_df[pop_colx] = pd.Series(index=anchor_df.index) \n",
    "\n",
    "contigs_identified = set()\n",
    "loc_cols2 = ['sseqid', 'sstart2', 'send2']\n",
    "\n",
    "for ix in anchor_df.index:\n",
    "    gene_, _var_ = list(anchor_df.loc[ix, ['locus_tag', 'variation_index']])\n",
    "    if type(_var_) != float:\n",
    "        locii, annot_ = list(var_df.loc[_var_, ['var_locii', 'Annotation Used for this Study']])\n",
    "        start_cut = locii[0]-1-8\n",
    "        if len(locii) == 1:\n",
    "            end_cut = locii[0]+8\n",
    "        else:\n",
    "            end_cut = locii[1]+8\n",
    "        ref_set = seq_array(full_genome[start_cut:end_cut][:16])\n",
    "    else:\n",
    "        ref_set = np.nan\n",
    "        annot_ = ngb_df.loc[ngb_df['locus_tag'] == gene_, 'product'].values[0]\n",
    "        locii = (0,0)\n",
    "    \n",
    "    blast_filt = blast_df.copy()[blast_df['locus_tag'] == gene_]\n",
    "    \n",
    "    # pull out populations with the homologous region\n",
    "    reg_by_iso = blast_filt[['genome_name', 'primer_position']].groupby('genome_name').agg(lambda x: len(set(x)))\n",
    "    good_isos = list(reg_by_iso[reg_by_iso['primer_position'] > 1 ].index)\n",
    "    anchor_df.loc[ix, 'has_homolog'] = \"@#$\".join(good_isos)\n",
    "    \n",
    "    # search for the wild type allele \n",
    "    blast_filt['has_wildtype'] = blast_filt[loc_cols2].apply(lambda x: find_wt(x, ref_set), axis=1)\n",
    "    wt_havers = set(blast_filt.loc[blast_filt['has_wildtype'] == True, 'genome_name'])\n",
    "    anchor_df.loc[ix, 'has_wildtype'] = \"@#$\".join(list(wt_havers))\n",
    "\n",
    "    # aggregate all the contigs that were identified \n",
    "    contigs_identified.update(list(blast_filt['sseqid'].unique()))\n",
    "    print(\"{}\\tGene = {}\\t Samples w/it = {}\\t Samples w/WT seq = {}\\t{}\".format(annot_, gene_, len(good_isos), \n",
    "                                                                             len(wt_havers), locii))\n",
    "for pop_colx in pop_cols:\n",
    "    nn_x = anchor_df[pop_colx].notnull()\n",
    "    anchor_df.loc[nn_x, pop_colx] = anchor_df.loc[nn_x, pop_colx].apply(lambda x: x.split(\"@#$\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene_ = 'CJ8421_RS06660'\n",
    "#blast_filt = blast_df[blast_df['locus_tag'] == gene_]\n",
    "#blast_filt.sample(1).T\n",
    "anchor_df[pop_cols].head(2).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we print a summary table where we look at regions of variation how often each region of variation is found in in primary and relapse isolates from both groups, as well as in the complete and reassembled versions of the reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innoc = 'crofts_innoc'\n",
    "ref_complete = 'CG8421'\n",
    "ref_sim = 'CG8421_sim'\n",
    "single_isos = ['Ref_Full', 'Ref_Sim', 'Crofts_Innoc'] \n",
    "single_keys = [ref_complete, ref_sim, innoc]\n",
    "\n",
    "relapse_crofts = list(metadata_df[metadata_df['primary_v_relapse_crofts'] == 'Relapse'].index)\n",
    "primary_crofts = [i for i in crofts_genomes if not i in single_keys and not i in relapse_crofts]\n",
    "print(len(crofts_genomes), len(primary_crofts), len(relapse_crofts), len(single_keys))\n",
    "\n",
    "peru_primary = list(metadata_df[metadata_df['primary_v_relapse_nocrofts'] == 'Primary'].index)\n",
    "peru_relapse = list(metadata_df[metadata_df['primary_v_relapse_nocrofts'] == 'Relapse'].index)\n",
    "\n",
    "group_names = ['Crofts_Primary', 'Crofts_Relapse', 'Iquitos_Primary', 'Iquitos_Relapse']\n",
    "group_keys = [primary_crofts, relapse_crofts, peru_primary, peru_relapse]\n",
    "\n",
    "procd_homologs = {}\n",
    "for anch_idx in anchor_df.index:\n",
    "    wt_isos, homolog_isos = anchor_df.loc[anch_idx, pop_cols]\n",
    "    assert type(wt_isos) == list == type(homolog_isos)\n",
    "    wt_isos, homolog_isos = set(wt_isos), set(homolog_isos)\n",
    "    rec_rows = {}\n",
    "    for g_n, g_k in zip(group_names, group_keys):\n",
    "        g_ks = set(g_k)\n",
    "        rec_rows[\"_\".join([g_n, 'w_Wildtype'])] = len(wt_isos.intersection(g_ks))\n",
    "        rec_rows[\"_\".join([g_n, 'wo_Wildtype'])] = len(g_ks) - len(wt_isos.intersection(g_ks))\n",
    "        rec_rows[\"_\".join([g_n, 'w_Region'])] = len(homolog_isos.intersection(g_ks))\n",
    "        rec_rows[\"_\".join([g_n, 'wo_Region'])] = len(g_ks) - len(homolog_isos.intersection(g_ks))\n",
    "        \n",
    "    for s_i, s_k in zip(single_isos, single_keys):\n",
    "        rec_rows[\"_\".join([s_i, 'has_Wildtype'])] = s_k in wt_isos\n",
    "        rec_rows[\"_\".join([s_i, 'Region_Count'])] = s_k in homolog_isos\n",
    "\n",
    "    procd_homologs[anch_idx] = rec_rows\n",
    "\n",
    "# Need to map the descriptive information from the original dataframe to here \n",
    "flipped_df = pd.DataFrame(procd_homologs).T\n",
    "e_anchor_df = pd.concat([anchor_df, flipped_df], axis=1)\n",
    "print(flipped_df.shape, anchor_df.shape, e_anchor_df.shape)\n",
    "\n",
    "flipped_df.to_csv('/Volumes/KeithSSD/variant-calling/misc_data/all_iso_all_region_processed.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we decide which of the two assemblies (primary or relapse) is more complete and then output a pair of columns in a text file of which assembly to use as a reference and which to library to map to it, in order to call variations. For the Croft's genomes, we will use the simulated and complete reference genomes, in order to reproduce their results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Iquitos relapse pairings from somewhere \n",
    "relpase_pair_file = '/Volumes/KeithSSD/CA_V3/data/09_Phylogenomics/relapse_dist_pairs_120719.txt'\n",
    "iquitos_pairings = pd.read_csv(relpase_pair_file, sep=\"\\t\", index_col=0)[['Pair Member 1', 'Pair Member 2']]\n",
    "all_genomes = set(blast_df['genome_name'])\n",
    "ip_links = {i:j for i, j in iquitos_pairings.values \n",
    "                if (i in all_genomes) and (j in all_genomes) and (not i in crofts_genomes)}\n",
    "\n",
    "# read in each desired contig set from each member of a pair\n",
    "print(\"{} contigs hit across all genomes\".format(len(contigs_identified)))\n",
    "assert len(set(contigs_identified) - set(all_cntg_df2['new_ids'])) == 0\n",
    "\n",
    "all_cntg_df2['source_genome'] = all_cntg_df2['new_ids'].apply(lambda x: x.split(\"-\")[0])\n",
    "all_cntg_df4 = all_cntg_df2.copy()[all_cntg_df2['new_ids'].isin(contigs_identified)]\n",
    "all_cntg_df4['seqlen'] = all_cntg_df4['sequence'].apply(len)\n",
    "\n",
    "contig_sum = all_cntg_df4[['source_genome', 'seqlen']].groupby('source_genome').agg(np.sum)['seqlen']\n",
    "\n",
    "assem_lib_pairs = []\n",
    "for cg in crofts_genomes:\n",
    "    assem_lib_pairs.append(('CG8421', cg))\n",
    "\n",
    "# whichever has a greater number of hits write out as the \"backbone\"\n",
    "peru_isos = set(peru_primary + peru_relapse)\n",
    "for k, v in ip_links.items():\n",
    "    peru_isos.remove(k); peru_isos.remove(v);\n",
    "    ixs = np.array([k, v])\n",
    "    anylz = np.array([contig_sum[k], contig_sum[v]])\n",
    "    assem_lib_pairs.append((ixs[anylz.argmax()], ixs[anylz.argmin()]))\n",
    "\n",
    "# create a table of the name of the backbone and the name of the reads to map against it\n",
    "mapping_pairs = pd.DataFrame(index=range(len(assem_lib_pairs)), data=assem_lib_pairs,\n",
    "                             columns=['reference_assembly', 'mapping_query'])\n",
    "\n",
    "# write a `snipper` file to do the mapping but alter it to find my contigs too\n",
    "mapping_pairs.to_csv(\"/Volumes/KeithSSD/variant-calling/misc_data/ref_map_pairs.txt\", sep=\"\\t\", \n",
    "                     header=None, index=None)\n",
    "mapping_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write contig sets from reference genomes into individual files in a directory \n",
    "ref_dir2 = \"/Volumes/KeithSSD/variant-calling/ref_fastas\"\n",
    "if not os.path.exists(ref_dir2):\n",
    "    os.mkdir(ref_dir2)\n",
    "\n",
    "ref_set = list(mapping_pairs['reference_assembly'].unique())\n",
    "out_paths = [os.path.join(ref_dir2, sn+\"_bb.fa\") for sn in ref_set]\n",
    "sub_dfs = [all_cntg_df4.copy()[all_cntg_df4['source_genome'] == sn] for sn in ref_set]\n",
    "for sn, of, sdf in zip(ref_set, out_paths, sub_dfs):\n",
    "    print(\"Writing {} recs for {} to {}\".format(len(sdf), sn, of))\n",
    "    table_to_fasta('new_ids', 'sequence', sdf, of)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've run prodigal to convert the FASTA files to genbank files and then Snippy to call variations between the reference backbone files and the libraries. We can proceed to read back the variants that were discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the SNP tables\n",
    "snp_result_dir = \"/Volumes/KeithSSD/variant-calling/snipout3_results\"\n",
    "snp_files = os.listdir(snp_result_dir)\n",
    "snp_paths = [os.path.join(snp_result_dir, f) for f in snp_files if f.endswith(\".2.tab\")]\n",
    "print(\"{} polymorphism reports detected\".format(len(snp_paths)))\n",
    "mp2 = [os.path.basename(i).replace(\".2.tab\", \"\").split(\"-\") + [i] for i in snp_paths]\n",
    "mapping_pairs2 = pd.DataFrame(data=mp2, columns=['ix', 'ref_assem', 'mapped_lib', 'result']).drop(['ix'], axis=1)\n",
    "results_df = {i:pd.read_csv(mapping_pairs2.loc[i, 'result'], sep=\"\\t\") for i in mapping_pairs2.index}\n",
    "\n",
    "bad_maps = (mapping_pairs2['ref_assem'] == 'CG8421') & (~mapping_pairs2['mapped_lib'].str.contains(\"crofts\"))\n",
    "bad_map_indexes = list(mapping_pairs2[bad_maps].index)\n",
    "bad_map_variants = np.array([results_df[i].shape[0] for i in bad_map_indexes])\n",
    "\n",
    "better_maps = (mapping_pairs2['ref_assem'] != 'CG8421') & (~mapping_pairs2['mapped_lib'].str.contains(\"crofts\"))\n",
    "better_map_indexes = list(mapping_pairs2[better_maps].index)\n",
    "better_map_variants = np.array([results_df[i].shape[0] for i in better_map_indexes])\n",
    "\n",
    "good_maps = (mapping_pairs2['ref_assem'] == 'CG8421') & (mapping_pairs2['mapped_lib'].str.contains(\"crofts\"))\n",
    "good_map_indexes = list(mapping_pairs2[good_maps].index)\n",
    "good_map_variants = np.array([results_df[i].shape[0] for i in good_map_indexes])\n",
    "\n",
    "best_maps = (mapping_pairs2['ref_assem'] == 'CG8421_sim') & (mapping_pairs2['mapped_lib'].str.contains(\"crofts\"))\n",
    "best_map_indexes = list(mapping_pairs2[best_maps].index)\n",
    "best_map_variants = np.array([results_df[i].shape[0] for i in best_map_indexes])\n",
    "\n",
    "print(\"Peru libs have a mean={:.2f}, std={:.2f} variations v. CG8421 ref (n={})\".format(bad_map_variants.mean(),\n",
    "                                                                                        bad_map_variants.std(),\n",
    "                                                                                        bad_map_variants.shape[0]))\n",
    "\n",
    "print(\"But a mean={:.2f}, std={:.2f} variations v. their own assems (n={})\".format(better_map_variants.mean(),\n",
    "                                                                                   better_map_variants.std(),\n",
    "                                                                                   better_map_variants.shape[0]))\n",
    "\n",
    "print(\"Crofts libs have a mean={:.2f}, std={:.2f} variations v. CG8421 ref (n={})\".format(good_map_variants.mean(),\n",
    "                                                                                          good_map_variants.std(),\n",
    "                                                                                          good_map_variants.shape[0]))\n",
    "\n",
    "print(\"and a mean={:.2f}, std={:.2f} variations v. CG8421 assem (n={})\".format(best_map_variants.mean(),\n",
    "                                                                               best_map_variants.std(),\n",
    "                                                                               best_map_variants.shape[0]))\n",
    "\n",
    "results_df = {k:v for k, v in results_df.items() if not k in bad_map_indexes}\n",
    "print(\"{} SNP reports retained\".format(len(results_df)))\n",
    "#assert set(mapping_pairs['reference_assembly']) == set(mapping_pairs2['ref_assem'])\n",
    "#assert set(mapping_pairs['mapping_query']) == set(mapping_pairs2['mapped_lib'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e_anchor_df['Variant Type'].unique())\n",
    "print(set([i for v in results_df.values() for i in v['TYPE'].unique()]))\n",
    "\n",
    "print(e_anchor_df['var_type'].unique())\n",
    "#list(results_df.values())[0]\n",
    "#e_anchor_df.iloc[0, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean up the blast hits \n",
    "select_blast = blast_df[blast_df['sseqid'].isin(contigs_identified)]\n",
    "\n",
    "# make variant types match \n",
    "e_anchor_df['var_type'] = e_anchor_df['Variant Type'].map({'Insertion':'ins', 'Deletion':'del', 'SNV':'snp'})\n",
    "\n",
    "def calc_evidence(x):\n",
    "    res_alt, res_evid = x\n",
    "    evid_nums = [int(i.split(\":\")[-1].strip()) for i in res_evid.split()]\n",
    "    assert res_alt == res_evid.split(\":\")[0].strip()\n",
    "    return float(evid_nums[0])/float(sum(evid_nums))\n",
    "\n",
    "# make one result df\n",
    "res_pile = []\n",
    "for a_res, res_dfx in results_df.items():\n",
    "    ref, query = list(mapping_pairs2.loc[a_res, ['ref_assem', 'mapped_lib']])\n",
    "    res_df = res_dfx.copy()\n",
    "    res_df['ref_assem'] = pd.Series(index=res_df.index, data=[ref]*len(res_df))\n",
    "    res_df['mapped_lib'] = pd.Series(index=res_df.index, data=[query]*len(res_df))\n",
    "    res_pile.append(res_df)\n",
    "\n",
    "all_results = pd.concat(res_pile, axis=0, ignore_index=True, verify_integrity=True)\n",
    "all_results['evidence_pct'] = all_results[['ALT', 'EVIDENCE']].apply(calc_evidence, axis=1)\n",
    "\n",
    "e_anchor_df['ALT'] = e_anchor_df['Variant Allele'].apply(lambda x: x.replace(\"-\", \"\") if type(x) == str else x)\n",
    "e_anchor_df['REF'] = e_anchor_df['Reference Sequence'].apply(lambda x: x.replace(\"-\", \"\") if type(x) == str else x)\n",
    "\n",
    "len_diff = lambda x: len(x[0]) - len(x[1]) if type(x[0]) != float else np.nan\n",
    "e_anchor_df['LEN_DIFF'] = e_anchor_df[['REF', 'ALT']].apply(len_diff, axis=1).astype(float)\n",
    "all_results['LEN_DIFF'] = all_results[['REF', 'ALT']].apply(len_diff, axis=1).astype(float)\n",
    "\n",
    "# determine whether it matches the location and type of a recorded mutation \n",
    "ncbi_ref = all_results['ref_assem'] == 'CG8421'\n",
    "for anch_idx in e_anchor_df.index:\n",
    "    b_vtype = (all_results['TYPE'] == e_anchor_df.loc[anch_idx, 'var_type'])\n",
    "    b_loc = (all_results['POS'] == e_anchor_df.loc[anch_idx, 'mut_start'])\n",
    "    b_loc = b_loc | (all_results['POS'] == e_anchor_df.loc[anch_idx, 'mut_start']+1)\n",
    "    b_loc = b_loc | (all_results['POS'] == e_anchor_df.loc[anch_idx, 'mut_start']+-1)\n",
    "    b_len = (all_results['LEN_DIFF'] == e_anchor_df.loc[anch_idx, 'LEN_DIFF'])\n",
    "    summ_1 = ncbi_ref & b_vtype & b_loc  & b_len\n",
    "    caught_pops = list(all_results.loc[summ_1, 'mapped_lib'].values)\n",
    "    exp_ = list(e_anchor_df.loc[anch_idx, ['REF', 'ALT', 'LEN_DIFF']])\n",
    "    obs_ = list(all_results.loc[summ_1, ['REF', 'ALT', 'LEN_DIFF']].apply(tuple, axis=1).unique())\n",
    "    print(anch_idx, summ_1.sum(), exp_, obs_, caught_pops)\n",
    "\n",
    "assert False\n",
    "    \n",
    "for res_row in res_df.index:\n",
    "    res_cntg, res_pos = res_df.loc[res_row, ['CHROM', 'POS']]\n",
    "    q1 = select_blast['sseqid'] == res_cntg\n",
    "    q2 = select_blast['sstart2'] <= res_pos\n",
    "    q3 = select_blast['send2'] >= res_pos\n",
    "\n",
    "    subhit_df = select_blast[((q1) & (q2) & (q3))]\n",
    "    if (subhit_df.shape[0] > 0) and (evidence_pct > 0.25) and ref == 'CG8421':\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # confirm whether its upstream, downstream, or in CDS based on primer name \n",
    "\n",
    "        # \n",
    "\n",
    "        # \n",
    "        print(subhit_df.T)\n",
    "        assert False\n",
    "        res_df.loc[res_row, 'primer_match'] = \", \".join(list(subhit_df['primer']))\n",
    "        l_cols2 = ['primer', 'send', 'sstart', 'qstart', 'qend', 'qlen']\n",
    "\n",
    "    # add \n",
    "    if ref == 'CG8421':\n",
    "        possible_affected = []\n",
    "        # if its within a CDS\n",
    "        within = (ngb_df['start'] <= res_pos) & (ngb_df['end'] >= res_pos)\n",
    "\n",
    "        # closest downstream watson strand encoded gene \n",
    "        downstream = (ngb_df['strand'] == 1) & ((ngb_df['start'] - res_pos) > 0)\n",
    "        if downstream.sum() > 0:\n",
    "            min_start = (ngb_df.loc[downstream, 'start'] - res_pos).astype(int).idxmin()\n",
    "            dist_to_start = ngb_df.loc[min_start, 'start'] - res_pos\n",
    "            res_df.loc[res_row, 'nearest_downstream'] = ngb_df.loc[min_start, 'locus_tag']\n",
    "            res_df.loc[res_row, 'dist_downstream'] = dist_to_start\n",
    "            if abs(dist_to_start) <= 50:\n",
    "                 possible_affected.append(res_df.loc[res_row, 'nearest_downstream'])\n",
    "\n",
    "        # closest upstream crick strand encoded gene\n",
    "        upstream = (ngb_df['strand'] == -1) & ((res_pos - ngb_df['end']) > 0)\n",
    "        if upstream.sum() > 0:\n",
    "            min_end = (res_pos - ngb_df.loc[upstream, 'end']).astype(int).idxmin()\n",
    "            dist_to_end = res_pos - ngb_df.loc[min_start, 'end']\n",
    "            res_df.loc[res_row, 'nearest_upstream'] = ngb_df.loc[min_end, 'locus_tag']\n",
    "            res_df.loc[res_row, 'dist_upstream'] = dist_to_end\n",
    "            if abs(dist_to_end) <= 50:\n",
    "                 possible_affected.append(res_df.loc[res_row, 'nearest_upstream'])\n",
    "\n",
    "        if within.sum() > 0:\n",
    "            possible_affected += list(ngb_df.loc[within, 'locus_tag'])\n",
    "            res_df.loc[res_row, 'affected_genes'] = \", \".join(possible_affected)\n",
    "            res_df.loc[res_row, 'within_gene'] = \", \".join(list(ngb_df.loc[within, 'locus_tag']))\n",
    "            #print(\"\\t\", res_row, res_cntg, res_pos, res_df.loc[res_row, 'within_gene'], list(ngb_df.loc[within, ['start', 'end', 'gene', 'product']].values))\n",
    "\n",
    "        aff_gene_cntr.update(possible_affected)\n",
    "\n",
    "results_df2[a_res] = res_df.copy()\n",
    "print(\"mapping {} to {}, gives {} matched variants {} affected genes\".format(ref, query, hit_counter, \n",
    "                                                                             len(aff_gene_cntr)))\n",
    "print(\"\\t{}\\n\".format(aff_gene_cntr))\n",
    "          \n",
    "\n",
    "drp_cols = ['AA_POS', 'EFFECT', 'FTYPE', 'GENE', 'LOCUS_TAG', 'NT_POS', 'PRODUCT', 'STRAND']\n",
    "snp_df = pd.concat(list(results_df2.values()), axis=0, ignore_index=True, sort=True).drop(drp_cols, axis=1)\n",
    "good_snps_df = snp_df.copy()[snp_df['primer_match'].notnull()]\n",
    "good_snps_df['primer_set'] = good_snps_df['primer_match'].apply(lambda x: set(x.split(\", \")))\n",
    "good_snps_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df.sample(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substandard_repeats = []\n",
    "for primer in sim_check_df.primer.unique():\n",
    "    matched_locations = sim_check_df[sim_check_df['primer'] == primer]\n",
    "    pident_adj = matched_locations['pident'] * ((matched_locations['qend'] - matched_locations['qstart'])/matched_locations['qend'])\n",
    "    best_locations = matched_locations[pident_adj == pident_adj.max()]\n",
    "    substandard_repeats += list(set(matched_locations.index) - set(best_locations.index))\n",
    "\n",
    "sim_check_df2 = sim_check_df.drop(substandard_repeats, axis=0)\n",
    "\n",
    "def convert_sim_locs(x):\n",
    "    sscdf = sim_check_df2[sim_check_df2['primer'] == x[0]]\n",
    "    if sscdf.shape[0] > 1:\n",
    "        return np.nan\n",
    "    elif sscdf.shape[0] == 1:\n",
    "        assert sscdf['qstart'].values[0] < sscdf['qend'].values[0]\n",
    "        if sscdf['sstart'].values[0] > sscdf['send'].values[0]:\n",
    "            return sscdf['sstart'].values[0]  - (x[1] - sscdf['qstart'].values[0])\n",
    "        else:\n",
    "            return sscdf['sstart'].values[0]  + (x[1] - sscdf['qstart'].values[0])\n",
    "    else:\n",
    "        raise ValueError(\"undefined contig mapped\")\n",
    "\n",
    "# \n",
    "falsePos, falseNeg, tru_pos = 0, 0, 0\n",
    "for cg in crofts_genomes:\n",
    "    this_lib = snp_df[snp_df['mapped_lib'] == cg]\n",
    "    positiveHits = this_lib[this_lib['ref_assem'] == 'CG8421']\n",
    "    print(cg)\n",
    "    questionableHits = this_lib.copy()[this_lib['ref_assem'] == 'CG8421_sim']\n",
    "    questionableHits['contig'] = questionableHits['CHROM'].apply(lambda x: x.split(\"-\")[1])\n",
    "    questionableHits['real_pos'] = questionableHits[['contig', 'POS']].apply(convert_sim_locs, axis=1)\n",
    "    print(questionableHits[['POS', 'real_pos', 'CG8421_POS', 'primer_match']])\n",
    "    questionableHits.sort_values(by='real_pos', inplace=True)\n",
    "    hitpool = set(positiveHits.index)\n",
    "    sourcepool = set(questionableHits.index)\n",
    "    for qh in questionableHits.index:\n",
    "        hit_dist = abs(positiveHits.loc[hitpool, 'POS'] - questionableHits.loc[qh, 'real_pos'])\n",
    "        closest_var = hit_dist[hit_dist == hit_dist.min()]\n",
    "        if len(closest_var) > 0:\n",
    "            closest_pos = positiveHits.loc[closest_var.index[0], 'POS']\n",
    "            other_candidates = [abs(questionableHits.loc[sp, 'real_pos']-closest_pos) for sp in sourcepool]\n",
    "            print(len(sourcepool), len(hitpool), closest_var.min(), min(other_candidates), closest_pos)        \n",
    "            if (closest_var.min() < 23) and (closest_var.min() <= min(other_candidates)):\n",
    "                hitpool.remove(closest_var.index[0])\n",
    "                assert len(closest_var) == 1\n",
    "                tru_pos += 1\n",
    "            else:\n",
    "                falsePos += 1\n",
    "        else:\n",
    "            falsePos += 1\n",
    "    \n",
    "        sourcepool.remove(qh)\n",
    "        \n",
    "    falseNeg += len(hitpool)\n",
    "    print(\"{} CG8421 {}, Sim {}, Left Over {}, TP {}, FP {}, FN {}\".format(cg, len(questionableHits), len(positiveHits), len(hitpool), \n",
    "                                                                           tru_pos, falsePos, falseNeg))\n",
    "\n",
    "precision = tru_pos/ (tru_pos + falsePos)\n",
    "recall = tru_pos/ (tru_pos + falseNeg)\n",
    "f1score=  2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(\"Using assemblies is has a F1 score of {:4f}\".format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_to_var_region = {}\n",
    "# map the region back to the variant index\n",
    "for e_ix, h, mr in e_anchor_df[['headers', 'mutation_rows']].reset_index().values:\n",
    "    primer_to_var_region[h] = set([e_ix])\n",
    "    \n",
    "# map the gene to the variation to the variant index\n",
    "for h, mr, sr in g_anchor_df[['headers', 'mutation_rows', 'support_rows']].values:\n",
    "    if mr != []:\n",
    "        e_ix = list(e_anchor_df[e_anchor_df['mutation_rows'].apply(lambda x: len(set(x).intersection(set(mr))) > 0 )].index)\n",
    "        primer_to_var_region[h] = set(e_ix)\n",
    "\n",
    "all_primers_possible = set(primer_to_var_region.keys())\n",
    "ontarget_hits = good_snps_df['primer_set'].apply(lambda x: len(x.intersection(all_primers_possible)) > 0)\n",
    "hit_snps = good_snps_df[ ontarget_hits & (good_snps_df['ref_assem'] == 'CG8421')]\n",
    "\n",
    "snp_locs = {i:set() for i in e_anchor_df.index}\n",
    "snp_populations = {i:set() for i in e_anchor_df.index}\n",
    "snp_ref = {i:set() for i in e_anchor_df.index}\n",
    "\n",
    "for ix in good_snps_df.index:\n",
    "    if good_snps_df.loc[ix, 'ref_assem'] != 'CG8421_sim':\n",
    "        these_matches = good_snps_df.loc[ix, 'primer_set']\n",
    "        these_anchors = [primer_to_var_region[i] for i in these_matches if i in primer_to_var_region.keys()]\n",
    "        these_anchors = set([i for j in these_anchors for i in j])\n",
    "        for anch in these_anchors:\n",
    "            snp_locs[anch].update([good_snps_df.loc[ix, 'CG8421_POS']])    \n",
    "            snp_populations[anch].update([good_snps_df.loc[ix, 'mapped_lib']])\n",
    "            snp_ref[anch].update([good_snps_df.loc[ix, 'ref_assem']])\n",
    "        \n",
    "flipped_df['SNPs_positions'] = pd.Series(snp_locs)\n",
    "flipped_df['SNPd_populations'] = pd.Series(snp_populations)\n",
    "flipped_df['SNP_references'] = pd.Series(snp_ref)\n",
    "\n",
    "group_keys = [['crofts_innoc'] , primary_crofts, relapse_crofts, peru_relapse]\n",
    "group_names = ['Innoc', 'Crofts_Primary', 'Crofts_Relapse', 'Iquitos_Relapse']\n",
    "new_col_names = [gn+\"_w_Variant\" for gn in group_names]\n",
    "new_col_names2 = [gn+\"_wo_Variant\" for gn in group_names]\n",
    "\n",
    "for gk, ncn, ncn2 in zip(group_keys, new_col_names, new_col_names2):\n",
    "    flipped_df[ncn2] = pd.Series({k:len(set(gk) - set(v)) for k, v in snp_populations.items()})\n",
    "    flipped_df[ncn] = pd.Series({k:len(set(gk).intersection(set(v))) for k, v in snp_populations.items()})\n",
    "\n",
    "expectation_cols = ['Matched_to_Crofts_Data', 'Missing_in_our_Data', 'Misreported_by_them']\n",
    "for ec in expectation_cols:\n",
    "    flipped_df[ec] = pd.Series(index=flipped_df.index)\n",
    "\n",
    "for ix in flipped_df.index:\n",
    "    snpd_pops = flipped_df.loc[ix, 'SNPd_populations']\n",
    "    pops_expctd = e_anchor_df.loc[ix, 'Samps_w_Mutation']\n",
    "    pops_not_expctd = e_anchor_df.loc[ix, 'Samps_off_Target']\n",
    "    flipped_df.loc[ix, expectation_cols[0]] = len(pops_expctd.intersection(snpd_pops))\n",
    "    flipped_df.loc[ix, expectation_cols[1]] = len(pops_expctd - snpd_pops)\n",
    "    flipped_df.loc[ix, expectation_cols[2]] = len(pops_not_expctd.intersection(snpd_pops))\n",
    "\n",
    "print(flipped_df[expectation_cols].sum())\n",
    "\n",
    "flipped_df.to_csv('/Volumes/KeithSSD/variant-calling/misc_data/Region_and_Snp_Data.txt', sep=\"\\t\")\n",
    "\n",
    "#valid_snps = hit_snps[['mapped_lib', 'primer_match', 'ref_assem']].groupby(['mapped_lib', 'primer_match'])\n",
    "#sim_and_not = lambda x: 'CG8421' in list(x) and 'CG8421_sim' in list(x)\n",
    "#print(valid_snps.agg(set))#.astype(int).sum().values[0])\n",
    "\n",
    "disp_cols = sorted(['SNPs_positions', 'SNPd_populations', 'SNP_references']+new_col_names + new_col_names2)\n",
    "flipped_df[disp_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_snps_df[good_snps_df['mapped_lib'] == 'crofts_1D2'].sort_values(by='POS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e_anchor_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
